{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "mount_file_id": "1usl804bO2xZScPOqcGWwGWdMFvtDsN4c",
      "authorship_tag": "ABX9TyPQrNUwrtBgLMr8UmIaud3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cyrinegharbi99/CyrineGharbi/blob/master/Moving_avg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9ezJmK3WPMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "#print(\"Tensorflow version \" + tf.version)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  \n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blOX4P4PX74P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler \n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjVFmWkGXBtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5e574f08-6ee7-4b5b-fe8f-5fee268e739b"
      },
      "source": [
        "#Listing the pairs\n",
        "pairs_list=['EURUSD','USDCAD','USDCHF','USDJPY','AUDUSD','GBPUSD','GBPAUD','GBPCAD','GBPCHF','GBPJPY','GBPNZD','NZDCHF','NZDJPY','NZDUSD','AUDCHF','AUDJPY','AUDNZD','CADCHF','CADJPY','CHFJPY','EURAUD','EURCAD','EURCHF','EURGBP','EURJPY','EURNZD','AUDCAD','NZDCAD']\n",
        "#Importing the data of all the inpput 3 pairs\n",
        "for pair in pairs_list :\n",
        "    globals()[\"forex_data_\" + pair] = pd.read_excel('/content/drive/My Drive/Syrine_folder/' + pair+'.xlsx')\n",
        "#Calculating the arithmetic perfY for each pair :\n",
        "for pair in pairs_list:\n",
        "    df =  globals()[\"forex_data_\" + pair]\n",
        "    df['perf' + pair] = ( df['close'] - df.shift(1,axis=0)['close'] )/df.shift(1,axis=0)['close']   \n",
        "#Deleting the rows containing NaN\n",
        "for pair in pairs_list :\n",
        "     globals()[\"forex_data_\" + pair] = globals()[\"forex_data_\" + pair].dropna(how = 'any')\n",
        "#Deleting the dates row :\n",
        "for pair in pairs_list :\n",
        "     globals()[\"forex_data_\" + pair].drop('date', axis=1, inplace=True)   \n",
        "#Normalize the data :\n",
        "for pair in pairs_list :\n",
        "    df= globals()[\"forex_data_\" + pair]\n",
        "    values = df.values\n",
        "    values = values.astype('float32')\n",
        "    # normalize features\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled = scaler.fit_transform(values)\n",
        "    \n",
        "#Creating the perf Dataframe containing all the perfY for Y in pairs_list :\n",
        "perf_df = pd.DataFrame()\n",
        "for pair in pairs_list:\n",
        "    perf_df = pd.concat([perf_df, globals()[\"forex_data_\" + pair][\"perf\" + pair]] , axis =1)\n",
        "    \n",
        "perf_df = perf_df.dropna(how = 'any')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aJq8GcwXIJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1200167c-1cd2-4930-cb28-d1c15ade0f0b"
      },
      "source": [
        "#Importing the data of all the 28 pairs\n",
        "for pair in pairs_list :\n",
        "    globals()[\"forex_test_data_\" + pair] = pd.read_excel('/content/drive/My Drive/Syrine_folder/Testing_data/' + pair+'.xlsx')\n",
        "    \n",
        "#Calculating the arithmetic perfY for each pair :\n",
        "for pair in pairs_list:\n",
        "    df =  globals()[\"forex_test_data_\" + pair]\n",
        "    df['perf' + pair] = ( df['close'] - df.shift(1,axis=0)['close'] )/df.shift(1,axis=0)['close']\n",
        "#Deleting the rows containing NaN\n",
        "for pair in pairs_list :\n",
        "     globals()[\"forex_test_data_\" + pair] = globals()[\"forex_test_data_\" + pair].dropna(how = 'any')\n",
        "        \n",
        "#Deleting the dates row :\n",
        "for pair in pairs_list :\n",
        "     globals()[\"forex_test_data_\" + pair].drop('date', axis=1, inplace=True)\n",
        "        \n",
        "#Normalize the data :\n",
        "for pair in pairs_list :\n",
        "    df= globals()[\"forex_test_data_\" + pair]\n",
        "    values = df.values\n",
        "    values = values.astype('float32')\n",
        "    # normalize features\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled = scaler.fit_transform(values)\n",
        "    \n",
        "#Creating the perf Dataframe containing all the perfY for Y in pairs_list :\n",
        "perf_test_df = pd.DataFrame()\n",
        "for pair in pairs_list:\n",
        "    perf_test_df = pd.concat([perf_df, globals()[\"forex_test_data_\" + pair][\"perf\" + pair]] , axis =1)\n",
        "    \n",
        "perf_test_df = perf_test_df.dropna(how = 'any')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjFYpUH9XMBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiKAVxy3XMEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43f5bd1d-e328-4954-c32b-5ca7b15b3077"
      },
      "source": [
        "timesteps = 18\n",
        "values = pairs_list \n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(perf_df)\n",
        "scaled_test = scaler.fit_transform(perf_test_df)\n",
        "\n",
        "reframed = series_to_supervised(scaled, timesteps, 1)\n",
        "reframed_test = series_to_supervised(scaled, timesteps, 1)\n",
        "reframed \n",
        "n_features = 28\n",
        "#Preparing the testing and training data\n",
        "values = reframed.values\n",
        "test_values = reframed_test.values\n",
        "print(values.shape)\n",
        "n_obs = n_features*timesteps\n",
        "n_train_time = 1868\n",
        "n_test_time = 1080\n",
        "train = values[:n_train_time, :]\n",
        "test = test_values[:n_test_time, :]\n",
        "# split into input and outputs\n",
        "train_X, train_y = train[:, :n_obs], train[:, -n_features:]\n",
        "#When given train_X[0] it should predict train_Y[0]\n",
        "test_X, test_y = test[:, :n_obs], test[:, -n_features:]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2377, 532)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn7sncmsYICz",
        "colab_type": "text"
      },
      "source": [
        "**Moving average on 6 timesteps :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_iLMdrzYHQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfUFIjzlXlT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e6fbc02-0f25-421a-ad71-09bf3a845966"
      },
      "source": [
        "# reshape input to be 3D [samples, timesteps, features] ==> A very key concept in LSTM training\n",
        "train_X = train_X.reshape((train_X.shape[0], timesteps, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], timesteps,n_features))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1868, 18, 28) (1868, 28) (1080, 18, 28) (1080, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKkBk64jZVFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "57d45448-1d3f-4330-d758-8168db5af789"
      },
      "source": [
        "train_X[1,:,1]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.58859272, 0.58264559, 0.34943303, 0.5282089 , 0.61916958,\n",
              "       0.57507104, 0.46670908, 0.31861697, 0.35869137, 0.55770411,\n",
              "       0.50853628, 0.54002007, 0.41846076, 0.48949636, 0.43987422,\n",
              "       0.45126061, 0.51124599, 0.49936464])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S499PTbYtsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_train=np.zeros((1868,28))\n",
        "for i in range(1868):\n",
        "  for j in range(28):\n",
        "    sum_train[i,j] =np.sum(train_X[i,:,j])/18"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBSomDcXd82F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c57d97c7-451d-439f-df9e-5596c2540229"
      },
      "source": [
        "sum[1,1]\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5405201446722577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT3UFmz7eHv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "09df2387-a58e-4d96-f997-5b6252a74059"
      },
      "source": [
        "train_X[1,:,1]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.58859272, 0.58264559, 0.34943303, 0.5282089 , 0.61916958,\n",
              "       0.57507104])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOwFshWPeA2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_avg = sum_train.reshape((sum_train.shape[0],1, n_features))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhvEOtg4f21g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a20b462e-7754-4419-929a-34b41061fed5"
      },
      "source": [
        "train_X_avg.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1868, 1, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pinZC1mbhstI",
        "colab_type": "text"
      },
      "source": [
        "**Moving average 6 on the testing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rDSvksFhrmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_test=np.zeros((1080,28))\n",
        "for i in range(1080):\n",
        "  for j in range(28):\n",
        "    sum_test[i,j] =np.sum(train_X[i,:,j])/18"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J2As3S3hySc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_X_avg = sum_test.reshape((sum_test.shape[0],1, n_features))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAq8BFdzhyYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "595344e1-d381-4985-abb8-73bfce5892da"
      },
      "source": [
        "test_X_avg.shape"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1080, 1, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGyzyStshyU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImdKQOprXMVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define model\n",
        "regressor = Sequential()\n",
        "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (1,28)))\n",
        "regressor.add(Dropout(0.2))\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))\n",
        "regressor.add(LSTM(units = 50))\n",
        "regressor.add(Dropout(0.2))\n",
        "regressor.add(Dense(units = 28))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhxVBs6xXMX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef17026a-0b0b-4063-f1bc-c95786d2f37e"
      },
      "source": [
        "regressor.compile(optimizer='adam', loss='mse')\n",
        "history = regressor.fit(train_X_avg, train_y, epochs=500, verbose=1,validation_data=(test_X_avg, test_y), shuffle = False) "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "59/59 [==============================] - 2s 26ms/step - loss: 0.1292 - val_loss: 0.0072\n",
            "Epoch 2/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0025\n",
            "Epoch 3/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0025\n",
            "Epoch 4/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0027\n",
            "Epoch 5/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0024\n",
            "Epoch 6/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0065 - val_loss: 0.0026\n",
            "Epoch 7/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0061 - val_loss: 0.0024\n",
            "Epoch 8/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0057 - val_loss: 0.0024\n",
            "Epoch 9/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0024\n",
            "Epoch 10/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0051 - val_loss: 0.0024\n",
            "Epoch 11/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0024\n",
            "Epoch 12/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0024\n",
            "Epoch 13/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0024\n",
            "Epoch 14/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0025\n",
            "Epoch 15/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0024\n",
            "Epoch 16/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0024\n",
            "Epoch 17/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0024\n",
            "Epoch 18/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0025\n",
            "Epoch 19/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0024\n",
            "Epoch 20/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0024\n",
            "Epoch 21/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0024\n",
            "Epoch 22/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0024\n",
            "Epoch 23/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0024\n",
            "Epoch 24/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0024\n",
            "Epoch 25/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0024\n",
            "Epoch 26/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0024\n",
            "Epoch 27/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0025\n",
            "Epoch 28/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0024\n",
            "Epoch 29/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 30/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0024\n",
            "Epoch 31/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 32/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 33/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 34/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 35/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0024\n",
            "Epoch 36/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0024\n",
            "Epoch 37/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0024\n",
            "Epoch 38/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0024\n",
            "Epoch 39/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 40/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0024\n",
            "Epoch 41/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 42/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 43/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 44/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 45/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 46/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 47/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 48/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 49/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 50/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 51/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 52/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 53/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 54/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 55/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 0.0024\n",
            "Epoch 56/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 57/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 58/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 59/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 60/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 61/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 62/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 63/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 64/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 65/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 66/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 67/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 68/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 69/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 70/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 71/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 72/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 73/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 74/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 75/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 76/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 77/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 78/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 79/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 80/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 81/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 82/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 83/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 84/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 85/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 86/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 87/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 88/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 89/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 90/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 91/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 92/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 93/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 94/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 95/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 96/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 97/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 98/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 99/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 100/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 101/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 102/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 103/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 104/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 105/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 106/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 107/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 108/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 109/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 110/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 111/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 112/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 113/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 114/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 115/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 116/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 117/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 118/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 119/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 120/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 121/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 122/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 123/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 124/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 125/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 126/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 127/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 128/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 129/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 130/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 131/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 132/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 133/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 134/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 135/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 136/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 137/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 138/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 139/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 140/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 141/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 142/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 143/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 144/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 145/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 146/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 147/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 148/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 149/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 150/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 151/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 152/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 153/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 154/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 155/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 156/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 157/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 158/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 159/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 160/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 161/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 162/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 163/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 164/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 165/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 166/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 167/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 168/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 169/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 170/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 171/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 172/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 173/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 174/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 175/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 176/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 177/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 178/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 179/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 180/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 181/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 182/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 183/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 184/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 185/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 186/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 187/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 188/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 189/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 190/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 191/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 192/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 193/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 194/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 195/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 196/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 197/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 198/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 199/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 200/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 201/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 202/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 203/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 204/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 205/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 206/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 207/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 208/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 209/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 210/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 211/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 212/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 213/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 214/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 215/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 216/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 217/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 218/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 219/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 220/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 221/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 222/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 223/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 224/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 225/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 226/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 227/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 228/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 229/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 230/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 231/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 232/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 233/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 234/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 235/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 236/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 237/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 238/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 239/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 240/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 241/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 242/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 243/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 244/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 245/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 246/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 247/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 248/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 249/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 250/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 251/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 252/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 253/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 254/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 255/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 256/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 257/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 258/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 259/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 260/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 261/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 262/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 263/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 264/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 265/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 266/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 267/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 268/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 269/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 270/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 271/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 272/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 273/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 274/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 275/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 276/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 277/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 278/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 279/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 280/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 281/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 282/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 283/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 284/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 285/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 286/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 287/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 288/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 289/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 290/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 291/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 292/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 293/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 294/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 295/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 296/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 297/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 298/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 299/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 300/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 301/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 302/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 303/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 304/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 305/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 306/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 307/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 308/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 309/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 310/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 311/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 312/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 313/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 314/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 315/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 316/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 317/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 318/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 319/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 320/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 321/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 322/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 323/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 324/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 325/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 326/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 327/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 328/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 329/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 330/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 331/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 332/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 333/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 334/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 335/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 336/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 337/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 338/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 339/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 340/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 341/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 342/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 343/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 344/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 345/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 346/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 347/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 348/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 349/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 350/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 351/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 352/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 353/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 354/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 355/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 356/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 357/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 358/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 359/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 360/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 361/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 362/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 363/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 364/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 365/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 366/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 367/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 368/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 369/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 370/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 371/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 372/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 373/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 374/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 375/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 376/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 377/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 378/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 379/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 380/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 381/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 382/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 383/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 384/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 385/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 386/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 387/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 388/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 389/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 390/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 391/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 392/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 393/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 394/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 395/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 396/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 397/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 398/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 399/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 400/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 401/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 402/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 403/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 404/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 405/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 406/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 407/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 408/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 409/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 410/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 411/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 412/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 413/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 414/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 415/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 416/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 417/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 418/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 419/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 420/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 421/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 422/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 423/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 424/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 425/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 426/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 427/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 428/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 429/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 430/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 431/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 432/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 433/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 434/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 435/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 436/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 437/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 438/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 439/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 440/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 441/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 442/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 443/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 444/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 445/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 446/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 447/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 448/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 449/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 450/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 451/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 452/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 453/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 454/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 455/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 456/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 457/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 458/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 459/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 460/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 461/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 462/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 463/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 464/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 465/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 466/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 467/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 468/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 469/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 470/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 471/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 472/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 473/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 474/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 475/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 476/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 477/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 478/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 479/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 480/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 481/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 482/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 483/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 484/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 485/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 486/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 487/500\n",
            "59/59 [==============================] - 0s 7ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 488/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 489/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 490/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 491/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 492/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 493/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 494/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 495/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 496/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 497/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 498/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 499/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 500/500\n",
            "59/59 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyTvI1kPgFQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8uDBajSXL_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3ad70add-56b2-4e06-ccf1-f2cfb14148a7"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "#plt.ylim(0,1e-2)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yQocCkWXYzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Forecasting accuracy function \n",
        "def calcul_accuracy(forecast, actual):\n",
        "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
        "    me = np.mean(forecast - actual)             # ME\n",
        "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
        "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
        "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
        "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
        "    mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n",
        "    maxs = np.amax(np.hstack([forecast[:,None], \n",
        "                              actual[:,None]]), axis=1)\n",
        "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
        "    accuracy = (np.count_nonzero(np.sign(forecast)== np.sign(actual)) /len(actual))*100 \n",
        "    smape = 100/len(actual) * np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
        "    #return({'mape':mape, 'me':me, 'mae': mae, 'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax, 'accuracy':accuracy})\n",
        "    return({'rmse':round(rmse,3), 'corr':round(corr,3), 'accuracy':round(accuracy,3), 'mpe': round(mpe,3), 'smape' : round(smape,3)})\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j65bi57yXY1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "28f2955b-4a95-460a-9a15-440984c50025"
      },
      "source": [
        "prediction = regressor.predict(test_X)\n",
        "prediction"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 1, 28) for input Tensor(\"lstm_4_input:0\", shape=(None, 1, 28), dtype=float32), but it was called on an input with incompatible shape (None, 18, 28).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5350816 , 0.49684212, 0.1653746 , ..., 0.54743415, 0.5584118 ,\n",
              "        0.54497707],\n",
              "       [0.5369053 , 0.4640954 , 0.16275272, ..., 0.54553133, 0.5559047 ,\n",
              "        0.54888564],\n",
              "       [0.5385294 , 0.44206145, 0.16068868, ..., 0.5443558 , 0.55430794,\n",
              "        0.5519176 ],\n",
              "       ...,\n",
              "       [0.53216535, 0.3717468 , 0.16411242, ..., 0.5388602 , 0.5454359 ,\n",
              "        0.55116916],\n",
              "       [0.52909565, 0.39410213, 0.16705729, ..., 0.5401387 , 0.54744405,\n",
              "        0.54634243],\n",
              "       [0.52760696, 0.41726828, 0.16880825, ..., 0.54178363, 0.54974747,\n",
              "        0.5434577 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYd_mJ3XY7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9da8f9c6-9387-49e9-85f8-59329f4d6041"
      },
      "source": [
        "accuracy=[]\n",
        "for i in range(1,24) :\n",
        "    accuracy = [accuracy,calcul_accuracy(test_y[i],prediction[i])]\n",
        "accuracy"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[[[[[[[[[[[[[[[[[[[[[[],\n",
              "                       {'accuracy': 100.0,\n",
              "                        'corr': 0.726,\n",
              "                        'mpe': 0.099,\n",
              "                        'rmse': 0.104,\n",
              "                        'smape': 13.607}],\n",
              "                      {'accuracy': 100.0,\n",
              "                       'corr': 0.789,\n",
              "                       'mpe': -0.036,\n",
              "                       'rmse': 0.081,\n",
              "                       'smape': 14.049}],\n",
              "                     {'accuracy': 100.0,\n",
              "                      'corr': 0.777,\n",
              "                      'mpe': 0.078,\n",
              "                      'rmse': 0.084,\n",
              "                      'smape': 13.274}],\n",
              "                    {'accuracy': 100.0,\n",
              "                     'corr': 0.893,\n",
              "                     'mpe': -0.093,\n",
              "                     'rmse': 0.063,\n",
              "                     'smape': 11.331}],\n",
              "                   {'accuracy': 100.0,\n",
              "                    'corr': 0.871,\n",
              "                    'mpe': 0.069,\n",
              "                    'rmse': 0.054,\n",
              "                    'smape': 7.912}],\n",
              "                  {'accuracy': 100.0,\n",
              "                   'corr': 0.82,\n",
              "                   'mpe': 0.023,\n",
              "                   'rmse': 0.055,\n",
              "                   'smape': 7.369}],\n",
              "                 {'accuracy': 100.0,\n",
              "                  'corr': 0.578,\n",
              "                  'mpe': -0.008,\n",
              "                  'rmse': 0.112,\n",
              "                  'smape': 15.92}],\n",
              "                {'accuracy': 100.0,\n",
              "                 'corr': 0.764,\n",
              "                 'mpe': -0.063,\n",
              "                 'rmse': 0.083,\n",
              "                 'smape': 11.425}],\n",
              "               {'accuracy': 100.0,\n",
              "                'corr': 0.612,\n",
              "                'mpe': 0.119,\n",
              "                'rmse': 0.121,\n",
              "                'smape': 19.196}],\n",
              "              {'accuracy': 100.0,\n",
              "               'corr': 0.936,\n",
              "               'mpe': -0.049,\n",
              "               'rmse': 0.038,\n",
              "               'smape': 7.213}],\n",
              "             {'accuracy': 100.0,\n",
              "              'corr': 0.75,\n",
              "              'mpe': 0.014,\n",
              "              'rmse': 0.063,\n",
              "              'smape': 9.426}],\n",
              "            {'accuracy': 100.0,\n",
              "             'corr': 0.699,\n",
              "             'mpe': -0.06,\n",
              "             'rmse': 0.084,\n",
              "             'smape': 11.187}],\n",
              "           {'accuracy': 100.0,\n",
              "            'corr': 0.575,\n",
              "            'mpe': -0.02,\n",
              "            'rmse': 0.093,\n",
              "            'smape': 12.774}],\n",
              "          {'accuracy': 100.0,\n",
              "           'corr': 0.631,\n",
              "           'mpe': -0.009,\n",
              "           'rmse': 0.092,\n",
              "           'smape': 13.682}],\n",
              "         {'accuracy': 100.0,\n",
              "          'corr': 0.825,\n",
              "          'mpe': 0.026,\n",
              "          'rmse': 0.062,\n",
              "          'smape': 10.958}],\n",
              "        {'accuracy': 100.0,\n",
              "         'corr': 0.85,\n",
              "         'mpe': 0.002,\n",
              "         'rmse': 0.046,\n",
              "         'smape': 6.883}],\n",
              "       {'accuracy': 100.0,\n",
              "        'corr': 0.889,\n",
              "        'mpe': -0.067,\n",
              "        'rmse': 0.051,\n",
              "        'smape': 8.742}],\n",
              "      {'accuracy': 100.0,\n",
              "       'corr': 0.885,\n",
              "       'mpe': 0.012,\n",
              "       'rmse': 0.04,\n",
              "       'smape': 6.6}],\n",
              "     {'accuracy': 100.0,\n",
              "      'corr': 0.948,\n",
              "      'mpe': 0.029,\n",
              "      'rmse': 0.028,\n",
              "      'smape': 4.59}],\n",
              "    {'accuracy': 100.0,\n",
              "     'corr': 0.739,\n",
              "     'mpe': -0.055,\n",
              "     'rmse': 0.072,\n",
              "     'smape': 11.832}],\n",
              "   {'accuracy': 100.0,\n",
              "    'corr': 0.615,\n",
              "    'mpe': -0.046,\n",
              "    'rmse': 0.092,\n",
              "    'smape': 14.09}],\n",
              "  {'accuracy': 100.0,\n",
              "   'corr': 0.918,\n",
              "   'mpe': -0.03,\n",
              "   'rmse': 0.038,\n",
              "   'smape': 6.466}],\n",
              " {'accuracy': 100.0,\n",
              "  'corr': 0.887,\n",
              "  'mpe': 0.023,\n",
              "  'rmse': 0.043,\n",
              "  'smape': 7.077}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxfQuFI5ne9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6a9a3e5-3349-4103-e41c-cb1448fe3943"
      },
      "source": [
        "prediction[2][2],test_y[2][2]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16068868, 0.11065551438051932)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm7VVFExXY5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}